{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functions\n",
    "\n",
    "from lambeq import BobcatParser\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMDB set\n",
    "\n",
    "## Process data, split into training/test\n",
    "imdb_data = pd.read_csv('IMDB Dataset.csv')\n",
    "## My PC is weak as fuck so truncate Dataset\n",
    "imdb_data = imdb_data[1:1000]\n",
    "size = int(len(imdb_data)*0.8)\n",
    "imdb_training = imdb_data.loc[:size].copy()\n",
    "imdb_testing = imdb_data.loc[size:].copy()\n",
    "\n",
    "## convert to numpy array\n",
    "train, trainlabels = imdb_training.iloc[:,0].to_numpy(), imdb_training.iloc[:,1].to_numpy()\n",
    "test, testlabels = imdb_testing.iloc[:,0].to_numpy(), imdb_testing.iloc[:,1].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['How unhappy  some dogs like it though', '-1'],\n",
       "       ['How unhappy  some dogs like it though', '-1'],\n",
       "       [\"Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \",\n",
       "        '-1'],\n",
       "       ...,\n",
       "       [' and took aol instant messanger at money in and more. ', '0'],\n",
       "       [\"isn't it terrible that we live in a world where we have to address anticipated criticism of our thoughts first? happy\",\n",
       "        '1'],\n",
       "       [\"I'll be as strong as I can but it's hard unhappy  xxx\", '-1']],\n",
       "      dtype='<U161')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Twitter dataset\n",
    "negative = np.loadtxt(\"processedNegative.csv\", delimiter=\",\", dtype = str).reshape(-1,1)\n",
    "neglabels = np.full(np.shape(negative),-1)\n",
    "neutral = np.loadtxt(\"processedNeutral.csv\", delimiter=\",\", dtype = str).reshape(-1,1)\n",
    "neutlabels = np.full(np.shape(neutral),0)\n",
    "positive = np.loadtxt(\"processedPositive.csv\", delimiter=\",\", dtype = str).reshape(-1,1)\n",
    "poslabels = np.full(np.shape(positive),1)\n",
    "\n",
    "negset = np.concatenate((negative,neglabels), axis=1)\n",
    "neutset = np.concatenate((neutral,neutlabels), axis=1)\n",
    "posset =  np.concatenate((positive,poslabels), axis=1)\n",
    "\n",
    "dataset = np.concatenate((negset,neutset,posset), axis=0)\n",
    "\n",
    "import random\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`tokenised` set to `False`, but variable `sentences` does not have type `List[str]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37828\\247113938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBobcatParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_cats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'N'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mraw_train_diagrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences2diagrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msuppress_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mraw_val_diagrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences2diagrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0msuppress_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\cleen\\Apps\\Anaconda3\\lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py\u001b[0m in \u001b[0;36msentences2diagrams\u001b[1;34m(self, sentences, tokenised, planar, suppress_exceptions, verbose)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m--> 161\u001b[1;33m         trees = self.sentences2trees(sentences,\n\u001b[0m\u001b[0;32m    162\u001b[0m                                      \u001b[0msuppress_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuppress_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                                      \u001b[0mtokenised\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenised\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\cleen\\Apps\\Anaconda3\\lib\\site-packages\\lambeq\\text2diagram\\bobcat_parser.py\u001b[0m in \u001b[0;36msentences2trees\u001b[1;34m(self, sentences, tokenised, suppress_exceptions, verbose)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0muntokenised_batch_type_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m                 raise ValueError('`tokenised` set to `False`, but variable '\n\u001b[0m\u001b[0;32m    350\u001b[0m                                  \u001b[1;34m'`sentences` does not have type '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                                  '`List[str]`.')\n",
      "\u001b[1;31mValueError\u001b[0m: `tokenised` set to `False`, but variable `sentences` does not have type `List[str]`."
     ]
    }
   ],
   "source": [
    "## Convert sentences to circuits w lambeq\n",
    "parser = BobcatParser(root_cats=('NP','N'),verbose = 'text')\n",
    "\n",
    "raw_train_diagrams = parser.sentences2diagrams(train,suppress_exceptions=True)\n",
    "raw_val_diagrams = parser.sentences2diagrams(test,  suppress_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba2d07656e6f23b1764554f46c91436454feee492d89e8db725dbc561fa5efa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
